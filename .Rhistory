VodMigData$Year <- as.numeric(as.character(VodMigData$Year))
VodMigData <- merge(VodMigData,years, by.x = "Year", by.y = "index", all.x=T)
knime.out<-VodMigData
install.packages("tidyr")
library(boxr)
box_auth(
client_id = 'vd00pwimq2qnwhfanxe9yu283nfbykbe',
client_secret = 'OMHrDtHq1Hz8WjalhUkLZNYQPyyq1S44'
)
?grepl
installed.package
installed.package("boxr")
library(lubridate)
year()
boxr
?boxr
install.packages("R.utils")
library("RJDBC", lib.loc="~/Library/R/3.3/library")
Sys.Date()
Sys.Date()+25
Sys.Date()+23
Sys.Date()+22
Sys.Date()+19
Sys.Date()
Sys.Date()+18
round()
round(4.55)
install.packages("searchable")
aa <- c("Bandwidth per Device", "adddd", "stb", "aaaa")
startsWith(aa, "band")
?startsWith
a <- c(1:5)
b <- c(6:10)
aa <- as.data.frame(rbind(a, b))
c <- c(4:9)
aa <- as.data.frame(rbind(a, b,c))
c <- c(4:8)
aa <- as.data.frame(rbind(a, b,c))
aa[, c(1,3,2)]
aa[, paste("ss", "1", sep = ".")] <- 1
aa
?sort
library(dplyr)
aa<- c("a", "B", "D", "e")
bb <- c(1:4)
cbind(aa, bb)
cc <- cbind(aa, bb)
cc <- as.data.frame(cbind(aa, bb))
cc
arrange(cc, aa)
library(tidyr)
gather(cc, 1:2)
gather(cc, 'col', 'val',1:2)
aa.2016 <- c(1:10)
bb.2017 <- c(2:11)
df <- as.data.frame(cbind(aa.2016, bb.2017))
df
require(tidyr)
gather(df, 'col', 'value', 1:2)
df1 <- gather(df, 'col', 'value', 1:2)
separate(df1, col, into = c('type', ))
separate(df1, col, into = c('type', 'year'), sep = ".")
?separate
separate(df1, col, into = c('type', 'year'), sep = "\\.")
?paste
install.packages("tools")
install.packages("tools")
install.packages("tools")
install.packages("tools")
install.packages("tools")
install.packages("Hmisc")
install.packages("scales")
install.packages("rJava")
install.packages("RJDBC")
install.packages("WriteXLS")
install.packages("xlxs")
install.packages("xlsx")
install.packages("sqldf")
install.packages("zoo")
install.packages("gsufn")
install.packages("proto")
install.packages("RSQLite")
install.packages("reshape")
install.packages("reshape2")
install.packages("doBy")
install.packages("DBI")
install.packages("pracma")
install.packages("data.table")
install.packages("tcltk")
install.packages("foreach")
install.packages("doParallel")
install.packages("parallel")
install.packages("sendmailR")
library(xlsx)
library(WriteXLS)
install.packages("lazyeval")
install.packages("ggplot2")
Sys.Date()
as.date('2016-11-19')
Sys.Date()-20
Sys.Date()-25
Sys.Date()-28
a <- '112.0'
grep(".",a)
grepl(".",a)
which(grepl(".",a))
which(grepl(".",a))
regexpr(".",a)
gregexpr(".",a)
sub(".0","",a)
?grepl
Med <- read.csv("./Documents/DataScience/ReproducibleResearch/WK1/Medical expense.csv", header = TRUE, sep = ",")
NY.Med <- subset(Med, Med$Provider.State == "NY")
graph <- ggplot(NY.Med, aes(log(Average.Covered.Charges), log(Average.Total.Payments)))
graph + geom_point(color = "black", size = 2, alpha = 0.5) + labs(title = "Relationship Between Mean Covered Charges and Mean Total Payments") + labs(x="Log of Mean Total Payments", y = "Log of Mean Covered Charges") + geom_smooth(method = "lm")
library(ggplot2)
NY.Med <- subset(Med, Med$Provider.State == "NY")
graph <- ggplot(NY.Med, aes(log(Average.Covered.Charges), log(Average.Total.Payments)))
graph + geom_point(color = "black", size = 2, alpha = 0.5) + labs(title = "Relationship Between Mean Covered Charges and Mean Total Payments") + labs(x="Log of Mean Total Payments", y = "Log of Mean Covered Charges") + geom_smooth(method = "lm")
ggsave("./Documents/DataScience/ReproducibleResearch/WK1/plot1.pdf", plot = last_plot(), scale = 1)
ggsave("./Documents/DataScience/ReproducibleResearch/WK1/plot1.pdf", plot = last_plot(), scale = 1, width = 7, height = 9)
graph <- ggplot(Med, aes(x = log(Average.Covered.Charges), y = log(Average.Total.Payments), colour = DRG.Definition))
graph + geom_point(alpha = 0.5) + geom_smooth(method = "lm", colour = "black") + facet_grid(DRG.Definition ~ Provider.State) + labs(title = "Relationship Between Mean Covered Charges and Mean Total Payments by DRG.Definition by State") + labs(x="Log of Mean Total Payments", y = "Log of Mean Covered Charges") + theme(legend.position = "bottom")
graph <- ggplot(Med, aes(x = log(Average.Covered.Charges), y = log(Average.Total.Payments), colour = DRG.Definition))
graph + geom_point(alpha = 0.5) + geom_smooth(method = "lm", colour = "black") + facet_grid(Provider.State ~ DRG.Definition) + labs(title = "Relationship Between Mean Covered Charges and Mean Total Payments by DRG.Definition by State") + labs(x="Log of Mean Total Payments", y = "Log of Mean Covered Charges") + theme(legend.position = "bottom")
ggsave("./Documents/DataScience/ReproducibleResearch/WK1/plot2.pdf", plot = last_plot(), scale = 1, width = 16, height = 9)
str(Med)
graph <- ggplot(Med, aes(x = log(Average.Covered.Charges), y = log(Average.Total.Payments), colour = DRG.Definition))
graph + geom_point(alpha = 0.5) + geom_smooth(method = "lm", colour = "black") + facet_grid(Provider.State ~ substr(as.character(DRG.Definition), 1, 3) + labs(title = "Relationship Between Mean Covered Charges and Mean Total Payments by DRG.Definition by State") + labs(x="Log of Mean Total Payments", y = "Log of Mean Covered Charges") + theme(legend.position = "bottom")
)
Med$DRG <- substr(as.character(Med$DRG.Definition), 1, 3)
str(Med)
graph <- ggplot(Med, aes(x = log(Average.Covered.Charges), y = log(Average.Total.Payments), colour = DRG.Definition))
graph + geom_point(alpha = 0.5) + geom_smooth(method = "lm", colour = "black") + facet_grid(Provider.State ~ DRG + labs(title = "Relationship Between Mean Covered Charges and Mean Total Payments by DRG.Definition by State") + labs(x="Log of Mean Total Payments", y = "Log of Mean Covered Charges") + theme(legend.position = "bottom")
graph <- ggplot(Med, aes(x = log(Average.Covered.Charges), y = log(Average.Total.Payments), colour = DRG.Definition))
graph + geom_point(alpha = 0.5) + geom_smooth(method = "lm", colour = "black") + facet_grid(Provider.State ~ DRG) + labs(title = "Relationship Between Mean Covered Charges and Mean Total Payments by DRG.Definition by State") + labs(x="Log of Mean Total Payments", y = "Log of Mean Covered Charges") + theme(legend.position = "bottom")
graph <- ggplot(Med, aes(x = log(Average.Covered.Charges), y = log(Average.Total.Payments), colour = DRG.Definition))
graph + geom_point(alpha = 0.5) + geom_smooth(method = "lm", colour = "black") + facet_grid(Provider.State ~ substr(as.character(DRG.Definition), 1, 3)) + labs(title = "Relationship Between Mean Covered Charges and Mean Total Payments by DRG.Definition by State") + labs(x="Log of Mean Total Payments", y = "Log of Mean Covered Charges") + theme(legend.position = "bottom")
graph <- ggplot(Med, aes(x = log(Average.Covered.Charges), y = log(Average.Total.Payments), colour = DRG.Definition))
graph + geom_point(alpha = 0.5) + geom_smooth(method = "lm", colour = "black") + facet_grid(substr(as.character(DRG.Definition), 1, 3) ~ Provider.State) + labs(title = "Relationship Between Mean Covered Charges and Mean Total Payments by DRG.Definition by State") + labs(x="Log of Mean Total Payments", y = "Log of Mean Covered Charges") + theme(legend.position = "bottom")
ggsave("./Documents/DataScience/ReproducibleResearch/WK1/plot2.pdf", plot = last_plot(), scale = 1, width = 20, height = 12)
graph + geom_point(alpha = 0.5) + geom_smooth(method = "lm", colour = "black") + facet_grid(Provider.State ~ substr(as.character(DRG.Definition), 1, 3)) + labs(title = "Relationship Between Mean Covered Charges and Mean Total Payments by DRG.Definition by State") + labs(x="Log of Mean Total Payments", y = "Log of Mean Covered Charges") + theme(legend.position = "bottom")
ggsave("./Documents/DataScience/ReproducibleResearch/WK1/plot2.pdf", plot = last_plot(), scale = 1, width = 25, height = 16)
ggsave("./Documents/DataScience/ReproducibleResearch/WK1/plot2.pdf", plot = last_plot(), scale = 1.5, width = 25, height = 16)
ggsave("./Documents/DataScience/ReproducibleResearch/WK1/plot2.pdf", plot = last_plot(), scale = 0.5, width = 25, height = 16)
ggsave("./Documents/DataScience/ReproducibleResearch/WK1/plot2.pdf", plot = last_plot(), scale = 0.75, width = 25, height = 16)
ggsave("./Documents/DataScience/ReproducibleResearch/WK1/plot2.pdf", plot = last_plot(), scale = 0.75, width = 20, height = 12)
ggsave("./Documents/DataScience/ReproducibleResearch/WK1/plot2.pdf", plot = last_plot(), scale = 0.8, width = 20, height = 12)
ggsave("./Documents/DataScience/ReproducibleResearch/WK1/plot2.pdf", plot = last_plot(), scale = 1, width = 20, height = 12)
ggsave("./Documents/DataScience/ReproducibleResearch/WK1/plot2.pdf", plot = last_plot(), scale = 1, width = 20, height = 12)
?aggregate
n1 <- "US HSD BAU"
substr(n1, "US ")
?substr
substr(n1, 4, length(n1))
substr(n1, 3, length(n1))
length(n1)
nchar(n1)
substr(n1, 4, nchar(n1)
)
install.packages("doBy")
library(sqldf)
install.packages('RODBC')
install.packages('RJDBC')
library(RJDBC)
library(RODBC)
library(RJDBC)
vDriver <- JDBC(driverClass = "com.vertica.jdbc.Driver", classPath = "Users/ywang203/Desktop/vertica-jdk5-6.1.3-0.jar")
getwd()
vDriver <- JDBC(driverClass = "com.vertica.jdbc.Driver", classPath = "./Desktop/vertica-jdk5-6.1.3-0.jar")
install.packages("rJava", type = 'source')
install.packages("rJava", type = "source")
install.packages('RJDBC')
library(RJDBC)
vDriver <- JDBC(driverClass = "com.vertica.jdbc.Driver", classPath = "./Desktop/vertica-jdk5-6.1.3-0.jar")
vertica <- dbConnect(vDriver, "jdbc:vertica://aggdmrapdb-wc-a03.sys.comcast.net:5433/aggRAP", "dashbusr", "id4d#sbd")
data <- dbGetQuery(vertica, "with consumption as
(
SELECT
year_month,
--to_char(consumption_period_date_dtl,'YYYY-MM-01') as year_month,
extract_site_from_cmts(cmts_fqdn) AS site_id,
device_id,
SUM(bytes_ds)/(2^30) AS DS_GB,
SUM(peak_bytes_ds)/(2^30) AS peak_period_DS_GB,
SUM(bytes_us)/(2^30) AS US_GB,
SUM(peak_bytes_us)/(2^30) AS peak_period_US_GB
FROM ipdr.HSD_MonthlyDeviceUsage
where year_month = '201701'
GROUP BY 1,2,3
), Cons_speed as
(
select
year_month,
site_id,
a.device_id,
b.device_ds_speed_id AS DS_speed_tier,
b.device_us_speed_id AS US_speed_tier,
DS_GB,
peak_period_DS_GB,
US_GB,
peak_period_US_GB
from
consumption a
-- group by 1,2,3,4,5,6,7,8,9
join ipdr.dash_device_dim b on b.device_id = a.device_id
)
select
year_month,
b.division_id,
b.region_id,
a.site_id,
DS_speed_tier as speed_tier,
'ds' as direction,
sum(DS_GB)      as gbytes,
sum(peak_period_DS_GB) as peak_period_gbytes
from Cons_speed a
join ipdr.PROD_Traffic_KPI_NetworkTopology b on b.site_id = a.site_id
group by 1,2,3,4,5,6
union
select
year_month,
b.division_id,
b.region_id,
a.site_id,
US_speed_tier as speed_tier,
'us' as direction,
sum(US_GB)      as gbytes,
sum(peak_period_US_GB) as peak_period_gbytes
from Cons_speed a
join ipdr.PROD_Traffic_KPI_NetworkTopology b on b.site_id = a.site_id
group by 1,2,3,4,5,6")
")
data <- dbGetQuery(vertica, "with consumption as
(
SELECT
year_month,
--to_char(consumption_period_date_dtl,'YYYY-MM-01') as year_month,
extract_site_from_cmts(cmts_fqdn) AS site_id,
device_id,
SUM(bytes_ds)/(2^30) AS DS_GB,
SUM(peak_bytes_ds)/(2^30) AS peak_period_DS_GB,
SUM(bytes_us)/(2^30) AS US_GB,
SUM(peak_bytes_us)/(2^30) AS peak_period_US_GB
FROM ipdr.HSD_MonthlyDeviceUsage
where year_month = '201701'
GROUP BY 1,2,3
), Cons_speed as
(
select
year_month,
site_id,
a.device_id,
b.device_ds_speed_id AS DS_speed_tier,
b.device_us_speed_id AS US_speed_tier,
DS_GB,
peak_period_DS_GB,
US_GB,
peak_period_US_GB
from
consumption a
-- group by 1,2,3,4,5,6,7,8,9
join ipdr.dash_device_dim b on b.device_id = a.device_id
)
select
year_month,
b.division_id,
b.region_id,
a.site_id,
DS_speed_tier as speed_tier,
'ds' as direction,
sum(DS_GB)      as gbytes,
sum(peak_period_DS_GB) as peak_period_gbytes
from Cons_speed a
join ipdr.PROD_Traffic_KPI_NetworkTopology b on b.site_id = a.site_id
group by 1,2,3,4,5,6
union
select
year_month,
b.division_id,
b.region_id,
a.site_id,
US_speed_tier as speed_tier,
'us' as direction,
sum(US_GB)      as gbytes,
sum(peak_period_US_GB) as peak_period_gbytes
from Cons_speed a
join ipdr.PROD_Traffic_KPI_NetworkTopology b on b.site_id = a.site_id
group by 1,2,3,4,5,6")
head(data)
Sys.Date()
install.packages("sparkR")
grepl
install.packages("SparkR")
install.packages('devtools')
devtools::install_github('apache/spark', subdir='R/pkg'')
devtools::install_github('apache/spark', subdir='R/pkg')
library(SparkR)
sparkR.session()
gsub()
?gsub
library(swirl)
install_from_swirl("Statistical Inference")
swirl()
(36-3)/36
deck
52
4/52
0
4*3/52
2/51
0.64
0.64
mypdf
integrate(mypdf, lower = 0, upper = 1.6)
sqrt(2)
0.997*0.001
0.003*0.988
0.985*0.001
0.015*0.999
0.000997/(0.000997+0.014985)
0.5
3.5
expect_dice
dice_high
expect_dice(dice_high)
expect_dice(dice_low)
(edh+edl)/2
integrate(myfunc, lower = 0, upper = 2)
spop
mean(spop)
allsam
apply(allsam, 1, mean)
smeans
mean(smeans)
dice_sqr
ex2_fair <- sum(dice_sqr*dice_fair)
ex2_fair <- sum(dice_sqr*dice_fair) - 3.5^2
ex2_fair - 3.5^2
sum(dice_sqr*dice_high) - edh^2
sd(apply(matrix(rnorm(10000),1000),1,mean))
1/sqrt(10)
1/sqrt(120)
sd(apply(matrix(rnorm(10000),1000),1,mean))
sd(apply(matrix(runif(10000),1000),1,mean))
2/sqrt(10)
sd(apply(matrix(rpois(10000),1000),1,mean))
sd(apply(matrix(rpois(10000,4),1000),1,mean))
1/2*sqrt(10)
1/(2*sqrt(10))
sd(apply(matrix(sample(0:1,10000,TRUE),1000),1,mean))
choose(5,3)*0.8^3*0.2^2 + choose(5,4)*0.8^4*0.2 + choose(5,5)*0.8^5
pbinom(2,5,0.8,FALSE)
qnorm(10)
qnorm(0.1)
0
qnorm(0.975, mean = 3, sd = 2)
3+2*1.96
pnorm(1200, mean = 1020, sd = 50,, lower.tail = FALSE)
pnorm(1200, mean = 1020, sd = 50,, lower.tail = FALSE)
pnorm((1200-1020)/50, lower.tail = FALSE)
qnorm(0.75, mean = 1020, sd = 50)
0.53
0.53
1-ppois(3, mean = 2.5*4)
1-ppois(quantitle(3), mean(2.5*4))
1-ppois(3, mean(2.5*4))
1-ppois(3, 2.5*4)
ppois(3, 2.5*4)
pbinom(5, 1000, 0.01)
ppois(5, 10)
ppois(5, 1000*0.01)
choose(4,5)
choose(5,4)
choose(5,4)*0.5^5+choose(5,5)*0.5^5
ppois(10, 5)
1-ppois(10, 5)
?ppois
?qpois
qpois(10,5)
dpois(10,5)
ppois(10,5)
ppois(10,15)
6/1.96
6/1.397
6/2.306
1.28/10
sqrt(1.28/10)
*2.101
sqrt(1.28/10)*2.101
sqrt(1.28/10)*2.101-2
sqrt(1.28/10)*2.10+2
1.96*sqrt(2.5/100)
1.96*sqrt(2.5/100)+2
2- 1.96*sqrt(2.5/100)
1.746*sqrt(1.5^2/9+1.8^2/9)
1.746*sqrt(1.5^2/9+1.8^2/9)-4
1.746*sqrt(1.5^2/9+1.8^2/9)+4
library(swirl)
swirl()
coinPlot(n= 10)
coinPlot(n= 10000)
qnorm(0.95)
0.6+c(-1,1)*qnorm(0.975)*sqrt(0.6*0.4/100)
x$conf.int
binom.test(60,100)
binom.test(60,100)$conf.int
mywald(0.2)
ACCompar(20)
lamb <- 5/94.32
lamb+c(-1,1)*qnorm(0.975)*sqrt(lamb/94.32)
poisson.test(5, 94.32)
poisson.test(5, 94.32)$conf.int
poisson.test(5, 94.32)$conf
library(swirl)
swirl()
swirl()
sqrt(5)
bc <- c(140, 138, 150, 148, 135)
wk2 <- c(132, 135, 151, 146, 130)
t.test(bc, wk2, paired = TRUE)
1100-23
0.25/sqrt(0.75*0.25/4)
?z.test
power.t.test(n = 100, delta = 1.66*0.004, sd = 0.04, sig.level = 0.1, type = "one.sample")
power.t.test(n = 100, delta = 1.66*0.004, sd = 0.04, sig.level = 0.05, type = "one.sample")
power.t.test(n = 100, delta = 0.01, sd = 0.04, sig.level = 0.05, type = "one.sample")
power.t.test(delta = 0.01, sd = 0.04, sig.level = 0.05, type = "one.sample")
power.t.test(delta = 0.01, sd = 0.04, power = 0.9, sig.level = 0.05, type = "one.sample")
power.t.test(delta = 0.01, sd = 0.04, power = 0.9, sig.level = 0.05, type = "two.sample", alternative = "one.sided")
power.t.test(delta = 0.01, sd = 0.04, power = 0.9, sig.level = 0.05, type = "one.sample", alternative = "one.sided")
power.t.test(delta = 0.01, sd = 0.04, sig.level = 0.05, type = "one.sample", alternative = "one.sided")
power.t.test(n = 100,delta = 0.01, sd = 0.04, sig.level = 0.05, type = "one.sample", alternative = "one.sided")
binom.test(3,4, p =0.5, alternative = "greater")
library(swirl)
swirl()
pValues
head(pValues)
sum(which(pValues < 0.05))
sum(pValues < 0.05)
sum(pValues < p.adjust(method = "bonferroni"))
sum(p.adjust(pValues, method = "bonferroni"))
sum(p.adjust(pValues, method = "bonferroni") < 0.05)
sum(p.adjust(pValues, method = "BH") < 0.05)
tail(trueStatus)
table(pValues2 < 0.05, trueStatus)
0
24/500
table(p.adjust(pValues2, method = "bonferroni") < 0.05, trueStatus)
table(p.adjust(pValues2, method = "BJ") < 0.05, trueStatus)
table(p.adjust(pValues2, method = "BH") < 0.05, trueStatus)
sum(c(1:6)*1/6)
print(g2)
head(sh)
nh
median(resampledMedians)
median(sh)
sam <- sample(fh, nh*B)
sam <- sample(fh, nh*B, replace = TRUE)
B
resame <- matrix(sam, B, nh)
resam <- matrix(sam, B, nh)
meds <- apply(resam, 1, fun = median)
meds <- apply(resam, 1, fun = median())
meds <- apply(resam, 1, median)
median(fh) - median(meds)
se(meds)
sd(meds)
sd(resampledMedians)
quantile(resampledMedians, c(0.025, 0.975))
quantile(meds, c(0.025, 0.975))
dim(InsectSprays)
names(InsectSprays)
range(Bdata$count)
range(Cdata$count)
BCcounts
group
testStat
obs <- testStat(BCcounts, group)
obs
mean(Bdata$count - Cdata$count)
sample(group)
perms <- sapply(1:10000, function(i) testStat(BCcounts, sample(group)))
perms > obs
mean(perms > obs)
testStat(DEcounts, group)
perms <- sapply(1:10000, function(i) testStat(DEcounts, sample(group)))
1135-841
qnorm(0.975)
install.packages("matrixStats")
library(matrixStats)
length(data_mean)
library("swirl")
swirl()
pValues
head(pValues)
sum(pValues<0.05)
0
knitr::knit("StatisticalInferenceProject.Rmd")
setwd("~/Documents/DataScience/Statistical Inference/Statistical Inference Project")
info()
bye()
setwd("~/Documents/DataScience/Statistical Inference/Statistical Inference Project")
knitr::knit("StatisticalInferenceProject.Rmd")
render("StatisticalInferenceProject.Rmd", output_file = "Statistical Inference Project.pdf")
library(knitr)
render("StatisticalInferenceProject.Rmd", output_file = "Statistical Inference Project.pdf")
library(rmarkdown)
render("StatisticalInferenceProject.Rmd", output_file = "Statistical Inference Project.pdf")
render("StatisticalInferenceProject.Rmd", output_file = "Statistical Inference Project.pdf")
knitr::knit("StatisticalInferenceProject1.Rmd")
knitr::knit("StatisticalInferenceProject2.Rmd")
